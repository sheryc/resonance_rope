{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4740740740740741,
      "acc_stderr": 0.04313531696750574,
      "acc_norm": 0.4740740740740741,
      "acc_norm_stderr": 0.04313531696750574
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5328947368421053,
      "acc_stderr": 0.04060127035236397,
      "acc_norm": 0.5328947368421053,
      "acc_norm_stderr": 0.04060127035236397
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5509433962264151,
      "acc_stderr": 0.030612730713641095,
      "acc_norm": 0.5509433962264151,
      "acc_norm_stderr": 0.030612730713641095
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5208333333333334,
      "acc_stderr": 0.041775789507399935,
      "acc_norm": 0.5208333333333334,
      "acc_norm_stderr": 0.041775789507399935
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145633
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4797687861271676,
      "acc_stderr": 0.03809342081273957,
      "acc_norm": 0.4797687861271676,
      "acc_norm_stderr": 0.03809342081273957
    },
    "hendrycksTest-college_physics": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.04220773659171453,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.04220773659171453
    },
    "hendrycksTest-computer_security": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.44680851063829785,
      "acc_stderr": 0.032500536843658404,
      "acc_norm": 0.44680851063829785,
      "acc_norm_stderr": 0.032500536843658404
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878151,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3253968253968254,
      "acc_stderr": 0.02413015829976262,
      "acc_norm": 0.3253968253968254,
      "acc_norm_stderr": 0.02413015829976262
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.040061680838488795,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488795
    },
    "hendrycksTest-global_facts": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6129032258064516,
      "acc_stderr": 0.02770935967503249,
      "acc_norm": 0.6129032258064516,
      "acc_norm_stderr": 0.02770935967503249
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3891625615763547,
      "acc_stderr": 0.03430462416103871,
      "acc_norm": 0.3891625615763547,
      "acc_norm_stderr": 0.03430462416103871
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6424242424242425,
      "acc_stderr": 0.03742597043806586,
      "acc_norm": 0.6424242424242425,
      "acc_norm_stderr": 0.03742597043806586
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6515151515151515,
      "acc_stderr": 0.03394853965156402,
      "acc_norm": 0.6515151515151515,
      "acc_norm_stderr": 0.03394853965156402
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7409326424870466,
      "acc_stderr": 0.03161877917935411,
      "acc_norm": 0.7409326424870466,
      "acc_norm_stderr": 0.03161877917935411
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.48205128205128206,
      "acc_stderr": 0.025334667080954942,
      "acc_norm": 0.48205128205128206,
      "acc_norm_stderr": 0.025334667080954942
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.25555555555555554,
      "acc_stderr": 0.026593939101844072,
      "acc_norm": 0.25555555555555554,
      "acc_norm_stderr": 0.026593939101844072
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5336134453781513,
      "acc_stderr": 0.03240501447690071,
      "acc_norm": 0.5336134453781513,
      "acc_norm_stderr": 0.03240501447690071
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.038020397601079024,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.038020397601079024
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7009174311926606,
      "acc_stderr": 0.019630417285415182,
      "acc_norm": 0.7009174311926606,
      "acc_norm_stderr": 0.019630417285415182
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.36574074074074076,
      "acc_stderr": 0.03284738857647207,
      "acc_norm": 0.36574074074074076,
      "acc_norm_stderr": 0.03284738857647207
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6862745098039216,
      "acc_stderr": 0.03256685484460388,
      "acc_norm": 0.6862745098039216,
      "acc_norm_stderr": 0.03256685484460388
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6751054852320675,
      "acc_stderr": 0.030486039389105296,
      "acc_norm": 0.6751054852320675,
      "acc_norm_stderr": 0.030486039389105296
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6412556053811659,
      "acc_stderr": 0.03219079200419995,
      "acc_norm": 0.6412556053811659,
      "acc_norm_stderr": 0.03219079200419995
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5648854961832062,
      "acc_stderr": 0.04348208051644858,
      "acc_norm": 0.5648854961832062,
      "acc_norm_stderr": 0.04348208051644858
    },
    "hendrycksTest-international_law": {
      "acc": 0.6694214876033058,
      "acc_stderr": 0.04294340845212094,
      "acc_norm": 0.6694214876033058,
      "acc_norm_stderr": 0.04294340845212094
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6481481481481481,
      "acc_stderr": 0.04616631111801713,
      "acc_norm": 0.6481481481481481,
      "acc_norm_stderr": 0.04616631111801713
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6257668711656442,
      "acc_stderr": 0.038020681028996146,
      "acc_norm": 0.6257668711656442,
      "acc_norm_stderr": 0.038020681028996146
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.30357142857142855,
      "acc_stderr": 0.04364226155841044,
      "acc_norm": 0.30357142857142855,
      "acc_norm_stderr": 0.04364226155841044
    },
    "hendrycksTest-management": {
      "acc": 0.6407766990291263,
      "acc_stderr": 0.04750458399041695,
      "acc_norm": 0.6407766990291263,
      "acc_norm_stderr": 0.04750458399041695
    },
    "hendrycksTest-marketing": {
      "acc": 0.7863247863247863,
      "acc_stderr": 0.026853450377009154,
      "acc_norm": 0.7863247863247863,
      "acc_norm_stderr": 0.026853450377009154
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.722860791826309,
      "acc_stderr": 0.016005636294122425,
      "acc_norm": 0.722860791826309,
      "acc_norm_stderr": 0.016005636294122425
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5924855491329479,
      "acc_stderr": 0.02645457814693151,
      "acc_norm": 0.5924855491329479,
      "acc_norm_stderr": 0.02645457814693151
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.29832402234636873,
      "acc_stderr": 0.015301840045129297,
      "acc_norm": 0.29832402234636873,
      "acc_norm_stderr": 0.015301840045129297
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5751633986928104,
      "acc_stderr": 0.028304576673141107,
      "acc_norm": 0.5751633986928104,
      "acc_norm_stderr": 0.028304576673141107
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6109324758842444,
      "acc_stderr": 0.027690337536485372,
      "acc_norm": 0.6109324758842444,
      "acc_norm_stderr": 0.027690337536485372
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.027125115513166858,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.027125115513166858
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.40425531914893614,
      "acc_stderr": 0.029275532159704725,
      "acc_norm": 0.40425531914893614,
      "acc_norm_stderr": 0.029275532159704725
    },
    "hendrycksTest-professional_law": {
      "acc": 0.4172099087353325,
      "acc_stderr": 0.012593959992906424,
      "acc_norm": 0.4172099087353325,
      "acc_norm_stderr": 0.012593959992906424
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.40441176470588236,
      "acc_stderr": 0.02981263070156974,
      "acc_norm": 0.40441176470588236,
      "acc_norm_stderr": 0.02981263070156974
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5261437908496732,
      "acc_stderr": 0.020200164564804588,
      "acc_norm": 0.5261437908496732,
      "acc_norm_stderr": 0.020200164564804588
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6454545454545455,
      "acc_stderr": 0.045820048415054174,
      "acc_norm": 0.6454545454545455,
      "acc_norm_stderr": 0.045820048415054174
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6,
      "acc_stderr": 0.03136250240935893,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03136250240935893
    },
    "hendrycksTest-sociology": {
      "acc": 0.6069651741293532,
      "acc_stderr": 0.0345368246603156,
      "acc_norm": 0.6069651741293532,
      "acc_norm_stderr": 0.0345368246603156
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.82,
      "acc_stderr": 0.038612291966536934,
      "acc_norm": 0.82,
      "acc_norm_stderr": 0.038612291966536934
    },
    "hendrycksTest-virology": {
      "acc": 0.43373493975903615,
      "acc_stderr": 0.03858158940685517,
      "acc_norm": 0.43373493975903615,
      "acc_norm_stderr": 0.03858158940685517
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7309941520467836,
      "acc_stderr": 0.034010526201040885,
      "acc_norm": 0.7309941520467836,
      "acc_norm_stderr": 0.034010526201040885
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=NousResearch/LLaMA-2-13B-YaRN-128K,use_accelerate=True,dtype=bfloat16,trust_remote_code=True",
    "num_fewshot": 5,
    "batch_size": "2",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}