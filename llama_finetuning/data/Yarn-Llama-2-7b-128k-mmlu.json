{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-anatomy": {
      "acc": 0.42962962962962964,
      "acc_stderr": 0.04276349494376599,
      "acc_norm": 0.42962962962962964,
      "acc_norm_stderr": 0.04276349494376599
    },
    "hendrycksTest-astronomy": {
      "acc": 0.375,
      "acc_stderr": 0.039397364351956274,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.039397364351956274
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4075471698113208,
      "acc_stderr": 0.030242233800854494,
      "acc_norm": 0.4075471698113208,
      "acc_norm_stderr": 0.030242233800854494
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4375,
      "acc_stderr": 0.04148415739394154,
      "acc_norm": 0.4375,
      "acc_norm_stderr": 0.04148415739394154
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.22,
      "acc_stderr": 0.0416333199893227,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.0416333199893227
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.37572254335260113,
      "acc_stderr": 0.036928207672648664,
      "acc_norm": 0.37572254335260113,
      "acc_norm_stderr": 0.036928207672648664
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237654,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "hendrycksTest-computer_security": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4127659574468085,
      "acc_stderr": 0.03218471141400351,
      "acc_norm": 0.4127659574468085,
      "acc_norm_stderr": 0.03218471141400351
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2719298245614035,
      "acc_stderr": 0.04185774424022056,
      "acc_norm": 0.2719298245614035,
      "acc_norm_stderr": 0.04185774424022056
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3931034482758621,
      "acc_stderr": 0.0407032901370707,
      "acc_norm": 0.3931034482758621,
      "acc_norm_stderr": 0.0407032901370707
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.022418042891113946,
      "acc_norm": 0.25396825396825395,
      "acc_norm_stderr": 0.022418042891113946
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.040735243221471255,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.040735243221471255
    },
    "hendrycksTest-global_facts": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.4129032258064516,
      "acc_stderr": 0.02800913812540039,
      "acc_norm": 0.4129032258064516,
      "acc_norm_stderr": 0.02800913812540039
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.03344283744280459,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.03344283744280459
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237101,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.04943110704237101
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6484848484848484,
      "acc_stderr": 0.037282069986826503,
      "acc_norm": 0.6484848484848484,
      "acc_norm_stderr": 0.037282069986826503
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3787878787878788,
      "acc_stderr": 0.03456088731993747,
      "acc_norm": 0.3787878787878788,
      "acc_norm_stderr": 0.03456088731993747
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.538860103626943,
      "acc_stderr": 0.03597524411734578,
      "acc_norm": 0.538860103626943,
      "acc_norm_stderr": 0.03597524411734578
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.34102564102564104,
      "acc_stderr": 0.024035489676335068,
      "acc_norm": 0.34102564102564104,
      "acc_norm_stderr": 0.024035489676335068
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.02646611753895991,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.02646611753895991
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3739495798319328,
      "acc_stderr": 0.03142946637883708,
      "acc_norm": 0.3739495798319328,
      "acc_norm_stderr": 0.03142946637883708
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.038020397601079024,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.038020397601079024
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5155963302752293,
      "acc_stderr": 0.02142689153920805,
      "acc_norm": 0.5155963302752293,
      "acc_norm_stderr": 0.02142689153920805
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25462962962962965,
      "acc_stderr": 0.029711275860005357,
      "acc_norm": 0.25462962962962965,
      "acc_norm_stderr": 0.029711275860005357
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5,
      "acc_stderr": 0.03509312031717982,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.03509312031717982
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.569620253164557,
      "acc_stderr": 0.032230171959375976,
      "acc_norm": 0.569620253164557,
      "acc_norm_stderr": 0.032230171959375976
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4977578475336323,
      "acc_stderr": 0.033557465352232634,
      "acc_norm": 0.4977578475336323,
      "acc_norm_stderr": 0.033557465352232634
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.45038167938931295,
      "acc_stderr": 0.04363643698524779,
      "acc_norm": 0.45038167938931295,
      "acc_norm_stderr": 0.04363643698524779
    },
    "hendrycksTest-international_law": {
      "acc": 0.5785123966942148,
      "acc_stderr": 0.04507732278775087,
      "acc_norm": 0.5785123966942148,
      "acc_norm_stderr": 0.04507732278775087
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.04830366024635331,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3496932515337423,
      "acc_stderr": 0.03746668325470021,
      "acc_norm": 0.3496932515337423,
      "acc_norm_stderr": 0.03746668325470021
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.32142857142857145,
      "acc_stderr": 0.044328040552915206,
      "acc_norm": 0.32142857142857145,
      "acc_norm_stderr": 0.044328040552915206
    },
    "hendrycksTest-management": {
      "acc": 0.3883495145631068,
      "acc_stderr": 0.0482572933735639,
      "acc_norm": 0.3883495145631068,
      "acc_norm_stderr": 0.0482572933735639
    },
    "hendrycksTest-marketing": {
      "acc": 0.6410256410256411,
      "acc_stderr": 0.03142616993791924,
      "acc_norm": 0.6410256410256411,
      "acc_norm_stderr": 0.03142616993791924
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.47,
      "acc_stderr": 0.050161355804659205,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.050161355804659205
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5696040868454662,
      "acc_stderr": 0.0177058687762924,
      "acc_norm": 0.5696040868454662,
      "acc_norm_stderr": 0.0177058687762924
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.47109826589595377,
      "acc_stderr": 0.026874085883518348,
      "acc_norm": 0.47109826589595377,
      "acc_norm_stderr": 0.026874085883518348
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.42483660130718953,
      "acc_stderr": 0.028304576673141117,
      "acc_norm": 0.42483660130718953,
      "acc_norm_stderr": 0.028304576673141117
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5530546623794212,
      "acc_stderr": 0.028237769422085335,
      "acc_norm": 0.5530546623794212,
      "acc_norm_stderr": 0.028237769422085335
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4845679012345679,
      "acc_stderr": 0.02780749004427619,
      "acc_norm": 0.4845679012345679,
      "acc_norm_stderr": 0.02780749004427619
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.36524822695035464,
      "acc_stderr": 0.028723863853281278,
      "acc_norm": 0.36524822695035464,
      "acc_norm_stderr": 0.028723863853281278
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3455019556714472,
      "acc_stderr": 0.012145303004087202,
      "acc_norm": 0.3455019556714472,
      "acc_norm_stderr": 0.012145303004087202
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.39705882352941174,
      "acc_stderr": 0.029722152099280072,
      "acc_norm": 0.39705882352941174,
      "acc_norm_stderr": 0.029722152099280072
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.43300653594771243,
      "acc_stderr": 0.020045442473324227,
      "acc_norm": 0.43300653594771243,
      "acc_norm_stderr": 0.020045442473324227
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5272727272727272,
      "acc_stderr": 0.04782001791380061,
      "acc_norm": 0.5272727272727272,
      "acc_norm_stderr": 0.04782001791380061
    },
    "hendrycksTest-security_studies": {
      "acc": 0.363265306122449,
      "acc_stderr": 0.03078905113903081,
      "acc_norm": 0.363265306122449,
      "acc_norm_stderr": 0.03078905113903081
    },
    "hendrycksTest-sociology": {
      "acc": 0.5223880597014925,
      "acc_stderr": 0.035319879302087305,
      "acc_norm": 0.5223880597014925,
      "acc_norm_stderr": 0.035319879302087305
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.68,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-virology": {
      "acc": 0.42168674698795183,
      "acc_stderr": 0.03844453181770917,
      "acc_norm": 0.42168674698795183,
      "acc_norm_stderr": 0.03844453181770917
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6432748538011696,
      "acc_stderr": 0.03674013002860954,
      "acc_norm": 0.6432748538011696,
      "acc_norm_stderr": 0.03674013002860954
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=NousResearch/LLaMA-2-7B-YaRN-128K,use_accelerate=True,dtype=bfloat16,trust_remote_code=True",
    "num_fewshot": 5,
    "batch_size": "2",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}